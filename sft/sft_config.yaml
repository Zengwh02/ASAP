model_name_or_path: model_name
attn_implementation: flash_attention_2

dataset_name: dataset_name

bf16: true
eval_strategy: "no"
learning_rate: 2.0e-5
num_train_epochs: 10
max_seq_length: 16384
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
logging_steps: 10
save_strategy: "epoch"
lr_scheduler_type: "cosine_with_min_lr"
warmup_ratio: 0.03
lr_scheduler_kwargs:
  min_lr_rate: 0.1
output_dir: output_dir
max_grad_norm: 0.2
seed: 42
